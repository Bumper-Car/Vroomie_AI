{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bumper-Car/Vroomie_AI/blob/main/LaneDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI7FYWcitPyK",
        "outputId": "3dbedc85-e92d-473e-bd55-c5771bcd58a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbRlohSHwtWH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Vroomie/training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXClFV5wrKWU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import time\n",
        "import copy\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.modules.loss import _Loss\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.nn import init\n",
        "\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-6OcokArNfS",
        "outputId": "f8cf6cd3-8207-4c73-813b-4188818d7605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wJ4D3uRtQPG"
      },
      "source": [
        "# AIHUBDrivingDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_extract(base_url, dataset_key, file_keys, api_key, output_dir=\"\"):\n",
        "    download_path = os.path.join(output_dir, \"download.tar\")\n",
        "    download_url = f\"{base_url}/{dataset_key}.do?fileSn={file_keys}\"\n",
        "\n",
        "    print(\"다운로드를 시작합니다...\")\n",
        "    result = subprocess.run(\n",
        "        [\"curl\", \"-L\", \"-C\", \"-\", \"-o\", download_path, \"-H\", f\"apikey:{api_key}\", download_url],\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        print(\"다운로드 성공.\")\n",
        "        # 압축 해제\n",
        "        subprocess.run([\"tar\", \"-xvf\", download_path], cwd=output_dir)\n",
        "\n",
        "        # 병합 함수 정의\n",
        "        def merge_parts(target_dir):\n",
        "            part_files = glob.glob(os.path.join(target_dir, \"*.part*\"))\n",
        "            if not part_files:\n",
        "                return\n",
        "            prefixes = sorted(set(f.split(\".part\")[0] for f in part_files))\n",
        "            for prefix in prefixes:\n",
        "                part_list = sorted(glob.glob(f\"{prefix}.part*\"))\n",
        "                with open(prefix, \"wb\") as outfile:\n",
        "                    for part in part_list:\n",
        "                        with open(part, \"rb\") as infile:\n",
        "                            outfile.write(infile.read())\n",
        "                        os.remove(part)\n",
        "                print(f\"병합 완료: {os.path.basename(prefix)}\")\n",
        "\n",
        "        # 하위 디렉토리 탐색\n",
        "        for root, dirs, files in os.walk(output_dir):\n",
        "            if any(\"part\" in f for f in files):\n",
        "                merge_parts(root)\n",
        "\n",
        "    # 4. 병합된 .zip 파일 해제\n",
        "    for root, dirs, files in os.walk(output_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".zip\"):\n",
        "                zip_path = os.path.join(root, file)\n",
        "                unzip_dir = os.path.splitext(zip_path)[0]\n",
        "                os.makedirs(unzip_dir, exist_ok=True)\n",
        "                subprocess.run([\"unzip\", \"-q\", zip_path, \"-d\", unzip_dir])\n",
        "                print(f\"압축 해제 완료: {zip_path}\")\n",
        "\n",
        "    # 5. tar 파일 삭제\n",
        "    os.remove(download_path)\n",
        "    print(\"모든 병합 및 압축 해제 완료.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"다운로드 실패\\n{result.stderr}\")\n",
        "        if os.path.exists(download_path):\n",
        "            with open(download_path, \"rb\") as f:\n",
        "                print(f.read().decode(errors=\"ignore\"))\n",
        "            os.remove(download_path)\n"
      ],
      "metadata": {
        "id": "PDUy_xq2NpkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AIHUB_API_KEY=E737AD1A-44BA-426B-92C6-D7BC8EBB5A87"
      ],
      "metadata": {
        "id": "JEVX26QOTL7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_and_extract(\n",
        "    base_url=\"https://api.aihub.or.kr\",\n",
        "    dataset_key=\"180\",\n",
        "    file_keys=\"38568\",\n",
        "    api_key=AIHUB_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "J2GRuobzRp1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_and_extract(\n",
        "    base_url=\"https://api.aihub.or.kr\",\n",
        "    dataset_key=\"180\",\n",
        "    file_keys=\"38569\",\n",
        "    api_key=AIHUB_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "-e9Rcs7LR2Uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_and_extract(\n",
        "    base_url=\"https://api.aihub.or.kr\",\n",
        "    dataset_key=\"180\",\n",
        "    file_keys=\"38576\",\n",
        "    api_key=AIHUB_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "jZRKRMR0R5hI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_and_extract(\n",
        "    base_url=\"https://api.aihub.or.kr\",\n",
        "    dataset_key=\"180\",\n",
        "    file_keys=\"38577\",\n",
        "    api_key=AIHUB_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "itIaOG7QR8aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_and_extract(\n",
        "    base_url=\"https://api.aihub.or.kr\",\n",
        "    dataset_key=\"180\",\n",
        "    file_keys=\"38578\",\n",
        "    api_key=AIHUB_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "HCqsnS44R-Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_and_extract(\n",
        "    base_url=\"https://api.aihub.or.kr\",\n",
        "    dataset_key=\"180\",\n",
        "    file_keys=\"38579\",\n",
        "    api_key=AIHUB_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "IFtTKHbPSCs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KUdX8QRzWCf"
      },
      "outputs": [],
      "source": [
        "def flatten_dataset(image_root, xml_root, target_img_dir, target_xml_dir):\n",
        "    os.makedirs(target_img_dir, exist_ok=True)\n",
        "    os.makedirs(target_xml_dir, exist_ok=True)\n",
        "\n",
        "    img_count, xml_count = 0, 0\n",
        "\n",
        "    for root, _, files in os.walk(image_root):\n",
        "        for file in files:\n",
        "            if file.endswith('.jpg'):\n",
        "                src = os.path.join(root, file)\n",
        "                dst = os.path.join(target_img_dir, file)\n",
        "                shutil.copy2(src, dst)\n",
        "                img_count += 1\n",
        "\n",
        "    for root, _, files in os.walk(xml_root):\n",
        "        for file in files:\n",
        "            if file.endswith('.xml'):\n",
        "                src = os.path.join(root, file)\n",
        "                dst = os.path.join(target_xml_dir, file)\n",
        "                shutil.copy2(src, dst)\n",
        "                xml_count += 1\n",
        "\n",
        "    print(f\"복사 완료: 이미지 {img_count}개, XML {xml_count}개\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUghfB9x0Ath"
      },
      "outputs": [],
      "source": [
        "# flatten_dataset(\n",
        "#     image_root=\"train/원천데이터\",\n",
        "#     xml_root=\"train/라벨링데이터\",\n",
        "#     target_img_dir=\"train/원천데이터_flat\",\n",
        "#     target_xml_dir=\"train/라벨링데이터_flat\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZMuYXY40Bnw"
      },
      "outputs": [],
      "source": [
        "# flatten_dataset(\n",
        "#     image_root=\"val/원천데이터\",\n",
        "#     xml_root=\"val/라벨링데이터\",\n",
        "#     target_img_dir=\"val/원천데이터_flat\",\n",
        "#     target_xml_dir=\"val/라벨링데이터_flat\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhSRa1WLtS1h"
      },
      "outputs": [],
      "source": [
        "LANE_LABELS = {\n",
        "    'Lane_White_Dash',\n",
        "    'Lane_White_Solid',\n",
        "    'Lane_Yellow_Dash',\n",
        "    'Lane_Yellow_Solid',\n",
        "    'Lane_Blue_Dash',\n",
        "    'Lane_Blue_Solid',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiboqKUgtUyd"
      },
      "outputs": [],
      "source": [
        "class AIHubDrivingDataset(Dataset):\n",
        "    def __init__(self, image_dir, label_dir, transform=None, target_transform=None, img_size=(320, 640)):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.img_size = img_size\n",
        "\n",
        "        self.label_list = sorted([f for f in os.listdir(label_dir) if f.endswith('.xml')])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        xml_name = self.label_list[idx]\n",
        "        xml_path = os.path.join(self.label_dir, xml_name)\n",
        "\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        img_name = root.find('filename').text\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        label_binary_image = np.zeros([image.shape[0], image.shape[1]], dtype=np.uint8)\n",
        "        label_instance_image = np.zeros([image.shape[0], image.shape[1]], dtype=np.uint8)\n",
        "\n",
        "        instance_id = 1\n",
        "\n",
        "        for line in root.findall('line'):\n",
        "            name = line.find('name')\n",
        "            if name is None or 'Lane' not in name.text:\n",
        "                continue\n",
        "\n",
        "            coords = list(line.find('controlPt'))\n",
        "            pts = []\n",
        "\n",
        "            for i in range(0, len(coords), 2):\n",
        "                x = int(coords[i].text)\n",
        "                y = int(coords[i+1].text)\n",
        "                pts.append((x, y))\n",
        "\n",
        "\n",
        "\n",
        "            if len(pts) >= 2:\n",
        "                pts = np.array([pts], np.int64)\n",
        "                cv2.polylines(label_binary_image, pts, isClosed=False, color=255, thickness=5)\n",
        "                cv2.polylines(label_instance_image, pts, isClosed=False, color=instance_id * 50 + 2, thickness=5)\n",
        "                instance_id += 1\n",
        "\n",
        "        image = Image.fromarray(image)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label_binary_image = self.target_transform(label_binary_image)\n",
        "            label_instance_image = self.target_transform(label_instance_image)\n",
        "\n",
        "        if label_binary_image.ndim == 2:\n",
        "            # Tensor → NumPy → BGR 변환 → 다시 Tensor\n",
        "            label_binary_image = label_binary_image.cpu().numpy().astype(np.uint8)\n",
        "            label_binary_image = cv2.cvtColor(label_binary_image, cv2.COLOR_GRAY2BGR)\n",
        "            label_binary_image = torch.from_numpy(label_binary_image)\n",
        "\n",
        "        label_binary = np.zeros([label_binary_image.shape[0], label_binary_image.shape[1]], dtype=np.uint8)\n",
        "        mask = np.where((label_binary_image.cpu().numpy() != [0, 0, 0]).all(axis=2))\n",
        "        label_binary[mask] = 1\n",
        "\n",
        "        return image, label_binary, label_instance_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29RjmZPCwVdB"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5Wnq_6luJF7"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    '''\n",
        "    Only consider two class now: foreground, background.\n",
        "    '''\n",
        "    def __init__(self, gamma=2, alpha=[0.5, 0.5], n_class=2, reduction='mean', device = DEVICE):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "        self.n_class = n_class\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        pt = F.softmax(input, dim=1)\n",
        "        pt = pt.clamp(min=0.000001,max=0.999999)\n",
        "        target_onehot = torch.zeros((target.size(0), self.n_class, target.size(1),target.size(2))).to(self.device)\n",
        "        loss = 0\n",
        "        for i in range(self.n_class):\n",
        "            target_onehot[:,i,...][target == i] = 1\n",
        "        for i in range(self.n_class):\n",
        "            loss -= self.alpha[i] * (1 - pt[:,i,...]) ** self.gamma * target_onehot[:,i,...] * torch.log(pt[:,i,...])\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            loss = torch.mean(loss)\n",
        "        elif self.reduction == 'sum':\n",
        "            loss = torch.sum(loss)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0DEeyG1uQzU"
      },
      "outputs": [],
      "source": [
        "class DiscriminativeLoss(_Loss):\n",
        "    def __init__(self, delta_var=0.5, delta_dist=1.5, norm=2, alpha=1.0, beta=1.0, gamma=0.001,\n",
        "                 usegpu=False, size_average=True):\n",
        "        super(DiscriminativeLoss, self).__init__(reduction='mean')\n",
        "        self.delta_var = delta_var\n",
        "        self.delta_dist = delta_dist\n",
        "        self.norm = norm\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "        self.usegpu = usegpu\n",
        "        assert self.norm in [1, 2]\n",
        "\n",
        "    def forward(self, input, target):\n",
        "\n",
        "        return self._discriminative_loss(input, target)\n",
        "\n",
        "    def _discriminative_loss(self, embedding, seg_gt):\n",
        "        batch_size, embed_dim, H, W = embedding.shape\n",
        "        embedding = embedding.reshape(batch_size, embed_dim, H*W)\n",
        "        seg_gt = seg_gt.reshape(batch_size, H*W)\n",
        "\n",
        "        var_loss = torch.tensor(0, dtype=embedding.dtype, device=embedding.device)\n",
        "        dist_loss = torch.tensor(0, dtype=embedding.dtype, device=embedding.device)\n",
        "        reg_loss = torch.tensor(0, dtype=embedding.dtype, device=embedding.device)\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            embedding_b = embedding[b]  # (embed_dim, H*W)\n",
        "            seg_gt_b = seg_gt[b]  # (H*W)\n",
        "\n",
        "            labels, indexs = torch.unique(seg_gt_b, return_inverse=True)\n",
        "            num_lanes = len(labels)\n",
        "            if num_lanes == 0:\n",
        "                _nonsense = embedding.sum()\n",
        "                _zero = torch.zeros_like(_nonsense)\n",
        "                var_loss = var_loss + _nonsense * _zero\n",
        "                dist_loss = dist_loss + _nonsense * _zero\n",
        "                reg_loss = reg_loss + _nonsense * _zero\n",
        "                continue\n",
        "\n",
        "            centroid_mean = []\n",
        "            for lane_idx in labels:\n",
        "                seg_mask_i = (seg_gt_b == lane_idx)\n",
        "\n",
        "                if not seg_mask_i.any():\n",
        "                    continue\n",
        "\n",
        "                embedding_i = embedding_b * seg_mask_i\n",
        "                mean_i = torch.sum(embedding_i, dim=1) / torch.sum(seg_mask_i)\n",
        "                centroid_mean.append(mean_i)\n",
        "                # ---------- var_loss -------------\n",
        "                var_loss = var_loss + torch.sum(F.relu(\n",
        "                    torch.norm(embedding_i[:,seg_mask_i] - mean_i.reshape(embed_dim, 1), dim=0) - self.delta_var) ** 2) / torch.sum(seg_mask_i) / num_lanes\n",
        "            centroid_mean = torch.stack(centroid_mean)  # (n_lane, embed_dim)\n",
        "\n",
        "            if num_lanes > 1:\n",
        "                centroid_mean1 = centroid_mean.reshape(-1, 1, embed_dim)\n",
        "                centroid_mean2 = centroid_mean.reshape(1, -1, embed_dim)\n",
        "\n",
        "                dist = torch.norm(centroid_mean1 - centroid_mean2, dim=2)   # shape (num_lanes, num_lanes)\n",
        "                dist = dist + torch.eye(num_lanes, dtype=dist.dtype,\n",
        "                                        device=dist.device) * self.delta_dist\n",
        "\n",
        "                # divided by two for double calculated loss above, for implementation convenience\n",
        "                dist_loss = dist_loss + torch.sum(F.relu(-dist + self.delta_dist) ** 2) / (\n",
        "                        num_lanes * (num_lanes - 1)) / 2\n",
        "\n",
        "            # reg_loss is not used in original paper\n",
        "            # reg_loss = reg_loss + torch.mean(torch.norm(centroid_mean, dim=1))\n",
        "\n",
        "        var_loss = var_loss / batch_size\n",
        "        dist_loss = dist_loss / batch_size\n",
        "        reg_loss = reg_loss / batch_size\n",
        "\n",
        "        return var_loss, dist_loss, reg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW2S9AreuDJp"
      },
      "outputs": [],
      "source": [
        "def compute_loss(net_output, binary_label, instance_label, loss_type = 'FocalLoss'):\n",
        "    k_binary = 10    #1.7\n",
        "    k_instance = 0.3\n",
        "    k_dist = 1.0\n",
        "\n",
        "    if(loss_type == 'FocalLoss'):\n",
        "        loss_fn = FocalLoss(gamma=2, alpha=[0.25, 0.75])\n",
        "    elif(loss_type == 'CrossEntropyLoss'):\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "    else:\n",
        "        # print(\"Wrong loss type, will use the default CrossEntropyLoss\")\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    binary_seg_logits = net_output[\"binary_seg_logits\"]\n",
        "    binary_loss = loss_fn(binary_seg_logits, binary_label)\n",
        "\n",
        "    pix_embedding = net_output[\"instance_seg_logits\"]\n",
        "    ds_loss_fn = DiscriminativeLoss(0.5, 1.5, 1.0, 1.0, 0.001)\n",
        "    var_loss, dist_loss, reg_loss = ds_loss_fn(pix_embedding, instance_label)\n",
        "    binary_loss = binary_loss * k_binary\n",
        "    var_loss = var_loss * k_instance\n",
        "    dist_loss = dist_loss * k_dist\n",
        "    instance_loss = var_loss + dist_loss\n",
        "    total_loss = binary_loss + instance_loss\n",
        "    out = net_output[\"binary_seg_pred\"]\n",
        "\n",
        "    return total_loss, binary_loss, instance_loss, out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uULDQuoPt5h7"
      },
      "outputs": [],
      "source": [
        "def train_model(model, optimizer, scheduler, dataloaders, dataset_sizes, device, loss_type = 'FocalLoss', num_epochs=25):\n",
        "    since = time.time()\n",
        "    training_log = {'epoch':[], 'training_loss':[], 'val_loss':[]}\n",
        "    best_loss = float(\"inf\")\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        training_log['epoch'].append(epoch)\n",
        "        current_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())\n",
        "        print('[{}] Epoch {}/{}'.format(current_time, epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_loss_b = 0.0\n",
        "            running_loss_i = 0.0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, binarys, instances in dataloaders[phase]:\n",
        "                inputs = inputs.type(torch.FloatTensor).to(device)\n",
        "                binarys = binarys.type(torch.LongTensor).to(device)\n",
        "                instances = instances.type(torch.FloatTensor).to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = compute_loss(outputs, binarys, instances, loss_type)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss[0].backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss[0].item() * inputs.size(0)\n",
        "                running_loss_b += loss[1].item() * inputs.size(0)\n",
        "                running_loss_i += loss[2].item() * inputs.size(0)\n",
        "\n",
        "            if phase == 'train':\n",
        "                if scheduler != None:\n",
        "                    scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            binary_loss = running_loss_b / dataset_sizes[phase]\n",
        "            instance_loss = running_loss_i / dataset_sizes[phase]\n",
        "            print('{} Total Loss: {:.4f} Binary Loss: {:.4f} Instance Loss: {:.4f}'.format(phase, epoch_loss, binary_loss, instance_loss))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'train':\n",
        "                training_log['training_loss'].append(epoch_loss)\n",
        "            if phase == 'val':\n",
        "                training_log['val_loss'].append(epoch_loss)\n",
        "                if epoch_loss < best_loss:\n",
        "                    best_loss = epoch_loss\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val_loss: {:4f}'.format(best_loss))\n",
        "    training_log['training_loss'] = np.array(training_log['training_loss'])\n",
        "    training_log['val_loss'] = np.array(training_log['val_loss'])\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, training_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjYY-OHGv4Ew"
      },
      "outputs": [],
      "source": [
        "class InitialBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(InitialBlock, self).__init__()\n",
        "        self.input_channel = in_ch\n",
        "        self.conv_channel = out_ch - in_ch\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch - in_ch, kernel_size = 3, stride = 2, padding=1),\n",
        "            nn.BatchNorm2d(out_ch - in_ch),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_branch = self.conv(x)\n",
        "        maxp_branch = self.maxpool(x)\n",
        "        return torch.cat([conv_branch, maxp_branch], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o_zqY98v7Dr"
      },
      "outputs": [],
      "source": [
        "class BottleneckModule(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, module_type, padding = 1, dilated = 0, asymmetric = 5, dropout_prob = 0):\n",
        "        super(BottleneckModule, self).__init__()\n",
        "        self.input_channel = in_ch\n",
        "        self.activate = nn.PReLU()\n",
        "\n",
        "        self.module_type = module_type\n",
        "        if self.module_type == 'downsampling':\n",
        "            self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.conv = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, kernel_size = 2, stride = 2),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.PReLU(),\n",
        "                nn.Conv2d(out_ch, out_ch, kernel_size = 3, stride=1, padding=padding),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.PReLU(),\n",
        "                nn.Conv2d(out_ch, out_ch, kernel_size = 1),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.PReLU(),\n",
        "                nn.Dropout2d(p=dropout_prob)\n",
        "            )\n",
        "        elif self.module_type == 'upsampling':\n",
        "            self.maxunpool = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, kernel_size = 1),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)    # Use upsample instead of maxunpooling\n",
        "            )\n",
        "\n",
        "            self.conv = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, kernel_size = 1),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.PReLU(),\n",
        "                nn.ConvTranspose2d(out_ch, out_ch, kernel_size=2, stride=2),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.PReLU(),\n",
        "                nn.Conv2d(out_ch, out_ch, kernel_size = 1),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.PReLU(),\n",
        "                nn.Dropout2d(p=dropout_prob)\n",
        "            )\n",
        "        elif self.module_type == 'regular':\n",
        "            self.conv = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, kernel_size = 1),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.PReLU(),\n",
        "                nn.Conv2d(out_ch, out_ch, kernel_size = 3, stride=1, padding=padding),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.PReLU(),\n",
        "                nn.Conv2d(out_ch, out_ch, kernel_size = 1),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.PReLU(),\n",
        "                nn.Dropout2d(p=dropout_prob)\n",
        "            )\n",
        "        elif self.module_type == 'asymmetric':\n",
        "            self.conv = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, kernel_size = 1),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.PReLU(),\n",
        "                nn.Conv2d(out_ch, out_ch, (asymmetric, 1), stride=1, padding=(padding, 0)),\n",
        "                nn.Conv2d(out_ch, out_ch, (1, asymmetric), stride=1, padding=(0, padding)),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.PReLU(),\n",
        "                nn.Conv2d(out_ch, out_ch, kernel_size = 1),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.PReLU(),\n",
        "                nn.Dropout2d(p=dropout_prob)\n",
        "            )\n",
        "        elif self.module_type == 'dilated':\n",
        "            self.conv = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, kernel_size = 1),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.PReLU(),\n",
        "                nn.Conv2d(out_ch, out_ch, kernel_size = 3, stride=1, padding=padding, dilation=dilated),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.PReLU(),\n",
        "                nn.Conv2d(out_ch, out_ch, kernel_size = 1),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.PReLU(),\n",
        "                nn.Dropout2d(p=dropout_prob)\n",
        "            )\n",
        "        else:\n",
        "            raise(\"Module Type error\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.module_type == 'downsampling':\n",
        "            conv_branch = self.conv(x)\n",
        "            maxp_branch = self.maxpool(x)\n",
        "            bs, conv_ch, h, w = conv_branch.size()\n",
        "            maxp_ch = maxp_branch.size()[1]\n",
        "            padding = torch.zeros(bs, conv_ch - maxp_ch, h, w).to(DEVICE)\n",
        "\n",
        "            maxp_branch = torch.cat([maxp_branch, padding], 1).to(DEVICE)\n",
        "            output = maxp_branch + conv_branch\n",
        "        elif self.module_type == 'upsampling':\n",
        "            conv_branch = self.conv(x)\n",
        "            maxunp_branch = self.maxunpool(x)\n",
        "            output = maxunp_branch + conv_branch\n",
        "        else:\n",
        "            output = self.conv(x) + x\n",
        "\n",
        "        return self.activate(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQVcNregv_oL"
      },
      "outputs": [],
      "source": [
        "def weights_init_kaiming(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if classname.find('Conv') != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        init.constant_(m.bias.data, 0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sz14bZRJvnGK"
      },
      "outputs": [],
      "source": [
        "class ENet_Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, in_ch=3, dropout_prob=0):\n",
        "        super(ENet_Encoder, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "\n",
        "        self.initial_block = InitialBlock(in_ch, 16)\n",
        "\n",
        "        self.bottleneck1_0 = BottleneckModule(16, 64, module_type = 'downsampling', padding = 1, dropout_prob = dropout_prob)\n",
        "        self.bottleneck1_1 = BottleneckModule(64, 64, module_type = 'regular', padding = 1, dropout_prob = dropout_prob)\n",
        "        self.bottleneck1_2 = BottleneckModule(64, 64, module_type = 'regular', padding = 1, dropout_prob = dropout_prob)\n",
        "        self.bottleneck1_3 = BottleneckModule(64, 64, module_type = 'regular', padding = 1, dropout_prob = dropout_prob)\n",
        "        self.bottleneck1_4 = BottleneckModule(64, 64, module_type = 'regular', padding = 1, dropout_prob = dropout_prob)\n",
        "\n",
        "        self.bottleneck2_0 = BottleneckModule(64, 128, module_type = 'downsampling', padding = 1, dropout_prob = dropout_prob)\n",
        "        self.bottleneck2_1 = BottleneckModule(128, 128, module_type = 'regular', padding = 1, dropout_prob = dropout_prob)\n",
        "        self.bottleneck2_2 = BottleneckModule(128, 128, module_type = 'dilated', padding = 2, dilated = 2, dropout_prob = dropout_prob)\n",
        "        self.bottleneck2_3 = BottleneckModule(128, 128, module_type = 'asymmetric', padding = 2, asymmetric=5, dropout_prob = dropout_prob)\n",
        "        self.bottleneck2_4 = BottleneckModule(128, 128, module_type = 'dilated', padding = 4, dilated = 4, dropout_prob = dropout_prob)\n",
        "        self.bottleneck2_5 = BottleneckModule(128, 128, module_type = 'regular', padding = 1, dropout_prob = dropout_prob)\n",
        "        self.bottleneck2_6 = BottleneckModule(128, 128, module_type = 'dilated', padding = 8, dilated = 8, dropout_prob = dropout_prob)\n",
        "        self.bottleneck2_7 = BottleneckModule(128, 128, module_type = 'asymmetric', padding = 2, asymmetric=5, dropout_prob = dropout_prob)\n",
        "        self.bottleneck2_8 = BottleneckModule(128, 128, module_type = 'dilated', padding = 16, dilated = 16, dropout_prob = dropout_prob)\n",
        "\n",
        "        self.bottleneck3_0 = BottleneckModule(128, 128, module_type = 'regular', padding = 1, dropout_prob = dropout_prob)\n",
        "        self.bottleneck3_1 = BottleneckModule(128, 128, module_type = 'dilated', padding = 2, dilated = 2, dropout_prob = dropout_prob)\n",
        "        self.bottleneck3_2 = BottleneckModule(128, 128, module_type = 'asymmetric', padding = 2, asymmetric=5, dropout_prob = dropout_prob)\n",
        "        self.bottleneck3_3 = BottleneckModule(128, 128, module_type = 'dilated', padding = 4, dilated = 4, dropout_prob = dropout_prob)\n",
        "        self.bottleneck3_4 = BottleneckModule(128, 128, module_type = 'regular', padding = 1, dropout_prob = dropout_prob)\n",
        "        self.bottleneck3_5 = BottleneckModule(128, 128, module_type = 'dilated', padding = 8, dilated = 8, dropout_prob = dropout_prob)\n",
        "        self.bottleneck3_6 = BottleneckModule(128, 128, module_type = 'asymmetric', padding = 2, asymmetric=5, dropout_prob = dropout_prob)\n",
        "        self.bottleneck3_7 = BottleneckModule(128, 128, module_type = 'dilated', padding = 16, dilated = 16, dropout_prob = dropout_prob)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                weights_init_kaiming(m)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                weights_init_kaiming(m)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial_block(x)\n",
        "\n",
        "        x = self.bottleneck1_0(x)\n",
        "        x = self.bottleneck1_1(x)\n",
        "        x = self.bottleneck1_2(x)\n",
        "        x = self.bottleneck1_3(x)\n",
        "        x = self.bottleneck1_4(x)\n",
        "\n",
        "        x = self.bottleneck2_0(x)\n",
        "        x = self.bottleneck2_1(x)\n",
        "        x = self.bottleneck2_2(x)\n",
        "        x = self.bottleneck2_3(x)\n",
        "        x = self.bottleneck2_4(x)\n",
        "        x = self.bottleneck2_5(x)\n",
        "        x = self.bottleneck2_6(x)\n",
        "        x = self.bottleneck2_7(x)\n",
        "        x = self.bottleneck2_8(x)\n",
        "\n",
        "        x = self.bottleneck3_0(x)\n",
        "        x = self.bottleneck3_1(x)\n",
        "        x = self.bottleneck3_2(x)\n",
        "        x = self.bottleneck3_3(x)\n",
        "        x = self.bottleneck3_4(x)\n",
        "        x = self.bottleneck3_5(x)\n",
        "        x = self.bottleneck3_6(x)\n",
        "        x = self.bottleneck3_7(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCRCV-GKvoZl"
      },
      "outputs": [],
      "source": [
        "class ENet_Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, out_ch=1, dropout_prob=0):\n",
        "        super(ENet_Decoder, self).__init__()\n",
        "\n",
        "\n",
        "        self.bottleneck4_0 = BottleneckModule(128, 64, module_type = 'upsampling', padding = 1, dropout_prob = dropout_prob)\n",
        "        self.bottleneck4_1 = BottleneckModule(64, 64, module_type = 'regular', padding = 1, dropout_prob = dropout_prob)\n",
        "        self.bottleneck4_2 = BottleneckModule(64, 64, module_type = 'regular', padding = 1, dropout_prob = dropout_prob)\n",
        "\n",
        "        self.bottleneck5_0 = BottleneckModule(64, 16, module_type = 'upsampling', padding = 1, dropout_prob = dropout_prob)\n",
        "        self.bottleneck5_1 = BottleneckModule(16, 16, module_type = 'regular', padding = 1, dropout_prob = dropout_prob)\n",
        "\n",
        "        self.fullconv = nn.ConvTranspose2d(16, out_ch, kernel_size=2, stride=2)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                weights_init_kaiming(m)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                weights_init_kaiming(m)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.bottleneck4_0(x)\n",
        "        x = self.bottleneck4_1(x)\n",
        "        x = self.bottleneck4_2(x)\n",
        "\n",
        "        x = self.bottleneck5_0(x)\n",
        "        x = self.bottleneck5_1(x)\n",
        "\n",
        "        x = self.fullconv(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTw7cB3ouu_P"
      },
      "outputs": [],
      "source": [
        "class LaneNet(nn.Module):\n",
        "    def __init__(self, in_ch = 3, arch=\"ENet\", output_size=(270, 480)):\n",
        "        super(LaneNet, self).__init__()\n",
        "        # no of instances for segmentation\n",
        "        self.no_of_instances = 3  # if you want to output RGB instance map, it should be 3.\n",
        "        print(\"Use {} as backbone\".format(arch))\n",
        "        self._arch = arch\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self._encoder = ENet_Encoder(in_ch)\n",
        "        self._encoder.to(DEVICE)\n",
        "\n",
        "        self._decoder_binary = ENet_Decoder(2)\n",
        "        self._decoder_instance = ENet_Decoder(self.no_of_instances)\n",
        "        self._decoder_binary.to(DEVICE)\n",
        "        self._decoder_instance.to(DEVICE)\n",
        "\n",
        "        self.relu = nn.ReLU().to(DEVICE)\n",
        "        self.sigmoid = nn.Sigmoid().to(DEVICE)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        c = self._encoder(input_tensor)\n",
        "        binary = self._decoder_binary(c)\n",
        "        instance = self._decoder_instance(c)\n",
        "\n",
        "        binary = F.interpolate(binary, size=self.output_size, mode='bilinear', align_corners=True)\n",
        "        instance = F.interpolate(instance, size=self.output_size, mode='bilinear', align_corners=True)\n",
        "\n",
        "        binary_seg_ret = torch.argmax(F.softmax(binary, dim=1), dim=1, keepdim=True)\n",
        "\n",
        "        pix_embedding = self.sigmoid(instance)\n",
        "\n",
        "        return {\n",
        "            'instance_seg_logits': pix_embedding,\n",
        "            'binary_seg_pred': binary_seg_ret,\n",
        "            'binary_seg_logits': binary\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiA2BjICwcqR"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeF_zhcNqa1_",
        "outputId": "5894433d-b9f4-4cca-c77d-ebdd1e09255b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use ENet as backbone\n",
            "25 epochs with 200 training samples\n",
            "\n",
            "[2025-05-26 06:05:09] Epoch 0/24\n",
            "----------\n",
            "train Total Loss: 0.8115 Binary Loss: 0.2095 Instance Loss: 0.6020\n",
            "val Total Loss: 0.4986 Binary Loss: 0.1414 Instance Loss: 0.3573\n",
            "\n",
            "[2025-05-26 06:08:55] Epoch 1/24\n",
            "----------\n",
            "train Total Loss: 0.3873 Binary Loss: 0.1128 Instance Loss: 0.2745\n",
            "val Total Loss: 0.3259 Binary Loss: 0.0937 Instance Loss: 0.2322\n",
            "\n",
            "[2025-05-26 06:09:23] Epoch 2/24\n",
            "----------\n",
            "train Total Loss: 0.2816 Binary Loss: 0.0802 Instance Loss: 0.2013\n",
            "val Total Loss: 0.2722 Binary Loss: 0.0703 Instance Loss: 0.2019\n",
            "\n",
            "[2025-05-26 06:09:52] Epoch 3/24\n",
            "----------\n",
            "train Total Loss: 0.2373 Binary Loss: 0.0617 Instance Loss: 0.1757\n",
            "val Total Loss: 0.2342 Binary Loss: 0.0562 Instance Loss: 0.1780\n",
            "\n",
            "[2025-05-26 06:10:22] Epoch 4/24\n",
            "----------\n",
            "train Total Loss: 0.2106 Binary Loss: 0.0499 Instance Loss: 0.1607\n",
            "val Total Loss: 0.2096 Binary Loss: 0.0465 Instance Loss: 0.1631\n",
            "\n",
            "[2025-05-26 06:10:51] Epoch 5/24\n",
            "----------\n",
            "train Total Loss: 0.1930 Binary Loss: 0.0417 Instance Loss: 0.1513\n",
            "val Total Loss: 0.1952 Binary Loss: 0.0399 Instance Loss: 0.1552\n",
            "\n",
            "[2025-05-26 06:11:21] Epoch 6/24\n",
            "----------\n",
            "train Total Loss: 0.1793 Binary Loss: 0.0356 Instance Loss: 0.1438\n",
            "val Total Loss: 0.1880 Binary Loss: 0.0340 Instance Loss: 0.1540\n",
            "\n",
            "[2025-05-26 06:11:50] Epoch 7/24\n",
            "----------\n",
            "train Total Loss: 0.1679 Binary Loss: 0.0309 Instance Loss: 0.1370\n",
            "val Total Loss: 0.1710 Binary Loss: 0.0310 Instance Loss: 0.1400\n",
            "\n",
            "[2025-05-26 06:12:19] Epoch 8/24\n",
            "----------\n",
            "train Total Loss: 0.1589 Binary Loss: 0.0271 Instance Loss: 0.1317\n",
            "val Total Loss: 0.1725 Binary Loss: 0.0273 Instance Loss: 0.1452\n",
            "\n",
            "[2025-05-26 06:12:48] Epoch 9/24\n",
            "----------\n",
            "train Total Loss: 0.1513 Binary Loss: 0.0242 Instance Loss: 0.1272\n",
            "val Total Loss: 0.1663 Binary Loss: 0.0246 Instance Loss: 0.1417\n",
            "\n",
            "[2025-05-26 06:13:17] Epoch 10/24\n",
            "----------\n",
            "train Total Loss: 0.1452 Binary Loss: 0.0217 Instance Loss: 0.1235\n",
            "val Total Loss: 0.1553 Binary Loss: 0.0227 Instance Loss: 0.1327\n",
            "\n",
            "[2025-05-26 06:13:46] Epoch 11/24\n",
            "----------\n",
            "train Total Loss: 0.1400 Binary Loss: 0.0197 Instance Loss: 0.1203\n",
            "val Total Loss: 0.1498 Binary Loss: 0.0214 Instance Loss: 0.1284\n",
            "\n",
            "[2025-05-26 06:14:15] Epoch 12/24\n",
            "----------\n",
            "train Total Loss: 0.1353 Binary Loss: 0.0179 Instance Loss: 0.1174\n",
            "val Total Loss: 0.1484 Binary Loss: 0.0207 Instance Loss: 0.1277\n",
            "\n",
            "[2025-05-26 06:14:45] Epoch 13/24\n",
            "----------\n",
            "train Total Loss: 0.1317 Binary Loss: 0.0169 Instance Loss: 0.1148\n",
            "val Total Loss: 0.1445 Binary Loss: 0.0184 Instance Loss: 0.1261\n",
            "\n",
            "[2025-05-26 06:15:14] Epoch 14/24\n",
            "----------\n",
            "train Total Loss: 0.1281 Binary Loss: 0.0155 Instance Loss: 0.1126\n",
            "val Total Loss: 0.1422 Binary Loss: 0.0176 Instance Loss: 0.1246\n",
            "\n",
            "[2025-05-26 06:15:43] Epoch 15/24\n",
            "----------\n",
            "train Total Loss: 0.1246 Binary Loss: 0.0148 Instance Loss: 0.1098\n",
            "val Total Loss: 0.1372 Binary Loss: 0.0172 Instance Loss: 0.1200\n",
            "\n",
            "[2025-05-26 06:16:12] Epoch 16/24\n",
            "----------\n",
            "train Total Loss: 0.1209 Binary Loss: 0.0135 Instance Loss: 0.1074\n",
            "val Total Loss: 0.1309 Binary Loss: 0.0172 Instance Loss: 0.1137\n",
            "\n",
            "[2025-05-26 06:16:42] Epoch 17/24\n",
            "----------\n",
            "train Total Loss: 0.1175 Binary Loss: 0.0126 Instance Loss: 0.1049\n",
            "val Total Loss: 0.1378 Binary Loss: 0.0168 Instance Loss: 0.1210\n",
            "\n",
            "[2025-05-26 06:17:11] Epoch 18/24\n",
            "----------\n",
            "train Total Loss: 0.1145 Binary Loss: 0.0119 Instance Loss: 0.1027\n",
            "val Total Loss: 0.1246 Binary Loss: 0.0157 Instance Loss: 0.1089\n",
            "\n",
            "[2025-05-26 06:17:40] Epoch 19/24\n",
            "----------\n",
            "train Total Loss: 0.1111 Binary Loss: 0.0112 Instance Loss: 0.0999\n",
            "val Total Loss: 0.1246 Binary Loss: 0.0147 Instance Loss: 0.1099\n",
            "\n",
            "[2025-05-26 06:18:10] Epoch 20/24\n",
            "----------\n",
            "train Total Loss: 0.1087 Binary Loss: 0.0108 Instance Loss: 0.0979\n",
            "val Total Loss: 0.1226 Binary Loss: 0.0145 Instance Loss: 0.1081\n",
            "\n",
            "[2025-05-26 06:18:39] Epoch 21/24\n",
            "----------\n",
            "train Total Loss: 0.1063 Binary Loss: 0.0100 Instance Loss: 0.0962\n",
            "val Total Loss: 0.1229 Binary Loss: 0.0149 Instance Loss: 0.1081\n",
            "\n",
            "[2025-05-26 06:19:09] Epoch 22/24\n",
            "----------\n",
            "train Total Loss: 0.1044 Binary Loss: 0.0096 Instance Loss: 0.0947\n",
            "val Total Loss: 0.1150 Binary Loss: 0.0145 Instance Loss: 0.1005\n",
            "\n",
            "[2025-05-26 06:19:39] Epoch 23/24\n",
            "----------\n",
            "train Total Loss: 0.1024 Binary Loss: 0.0095 Instance Loss: 0.0928\n",
            "val Total Loss: 0.1091 Binary Loss: 0.0129 Instance Loss: 0.0961\n",
            "\n",
            "[2025-05-26 06:20:08] Epoch 24/24\n",
            "----------\n",
            "train Total Loss: 0.0996 Binary Loss: 0.0088 Instance Loss: 0.0908\n",
            "val Total Loss: 0.1142 Binary Loss: 0.0142 Instance Loss: 0.1000\n",
            "\n",
            "Training complete in 15m 29s\n",
            "Best val_loss: 0.109078\n",
            "Training log saved: output/training_log.csv\n",
            "Model saved: output/best_model.pth\n"
          ]
        }
      ],
      "source": [
        "save_path = 'output'\n",
        "dataset = ''\n",
        "batch_size = 4\n",
        "model_type = 'ENet'\n",
        "learning_rate = 0.0001\n",
        "training_epochs = 25\n",
        "loss_type = 'FocalLoss'\n",
        "(resize_height, resize_width) = (270, 480)\n",
        "\n",
        "\n",
        "if not os.path.isdir(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "# 경로 설정\n",
        "train_image_dir = os.path.join(dataset, 'train/원천데이터_flat')\n",
        "train_label_dir = os.path.join(dataset, 'train/라벨링데이터_flat')\n",
        "val_image_dir = os.path.join(dataset, 'val/원천데이터_flat')\n",
        "val_label_dir = os.path.join(dataset, 'val/라벨링데이터_flat')\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((resize_height, resize_width)),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((resize_height, resize_width)),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "target_transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda mask: torch.from_numpy(\n",
        "        cv2.resize(mask, (resize_width, resize_height), interpolation=cv2.INTER_NEAREST)\n",
        "    ).long())\n",
        "])\n",
        "\n",
        "# Dataset 및 DataLoader 정의\n",
        "train_dataset = AIHubDrivingDataset(\n",
        "    image_dir=train_image_dir,\n",
        "    label_dir=train_label_dir,\n",
        "    transform=data_transforms['train'],\n",
        "    target_transform=target_transform\n",
        ")\n",
        "val_dataset = AIHubDrivingDataset(\n",
        "    image_dir=val_image_dir,\n",
        "    label_dir=val_label_dir,\n",
        "    transform=data_transforms['val'],\n",
        "    target_transform=target_transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "dataloaders = {'train': train_loader, 'val': val_loader}\n",
        "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
        "\n",
        "# 모델 초기화\n",
        "model = LaneNet(arch=model_type)\n",
        "model.to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "print(f\"{training_epochs} epochs with {len(train_dataset)} training samples\\n\")\n",
        "\n",
        "model, log = train_model(\n",
        "    model,\n",
        "    optimizer,\n",
        "    scheduler=None,\n",
        "    dataloaders=dataloaders,\n",
        "    dataset_sizes=dataset_sizes,\n",
        "    device=DEVICE,\n",
        "    loss_type=loss_type,\n",
        "    num_epochs=training_epochs\n",
        ")\n",
        "\n",
        "# 로그 저장\n",
        "df = pd.DataFrame(log)\n",
        "train_log_save_filename = os.path.join(save_path, 'training_log.csv')\n",
        "df.to_csv(train_log_save_filename, index=False, encoding='utf-8')\n",
        "print(f\"Training log saved: {train_log_save_filename}\")\n",
        "\n",
        "# 모델 저장\n",
        "model_save_filename = os.path.join(save_path, 'best_model.pth')\n",
        "torch.save(model.state_dict(), model_save_filename)\n",
        "print(f\"Model saved: {model_save_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79_ovnqE6Kz6"
      },
      "outputs": [],
      "source": [
        "!cp output/best_model.pth /content/drive/MyDrive/Vroomie/inference/model/lane_detection_model.pth"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHSANo/Fx4pehySrkUvGjW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
